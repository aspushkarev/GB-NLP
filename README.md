# GB-NLP
Studying at GB on the course Introducing in NLP
1. Intro. Introducing to NLP;
2. feature_space. Creating a feature space;
3. Embedding. Word2Vec, FastText;
4. thematic modeling. Thematic modeling.
   LDA model and attempt implement BigARTM Library in Google Colab;
5. NER. Parts-of-Speech, NER;
6. Sentiment_Analysis. Sentiment Analysis;
7. CNN. Convolutional Neural Networks for Text Analysis;
8. CNN_RNN_LSTM_GRU. Compare CNN, RNN, LSTM, GURU;
9. text_generation. Trained model based on the poem by Russian poet Alexander Pushkin “Eugene Onegin”;
10. seq2seq. Neural machine translation with attention;
11. Transformer1. Understand the translation model (with the attention) how it works, launch it to translate from Russian into English;
12. Summarization. Implement the task of text summarization;
13. BERT. Task of paraphrase, text2text generation by using pre-trained model BERT;
14. NER_GPT_T5. Named entity recognition. GPT. T5;
15. bot. Added tg_bot. But not it work. I don`t have enough time to implement it
