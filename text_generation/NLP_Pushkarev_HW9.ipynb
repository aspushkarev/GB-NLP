{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Text generation\n",
        "\n",
        "Understand to text generation model, to build yourself dataset or take dataset from webinar and teach text generation model"
      ],
      "metadata": {
        "id": "5XefMWKjKlcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2KcQQU7eC3DO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import GRU, LSTM, Embedding, Dense\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('evgenyi_onegin.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rT9VOFXS1w5",
        "outputId": "c68c8b03-01d7-45e1-e830-02d0c45c4667"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 286984 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ukp6znaVC2z",
        "outputId": "ecc34030-a8a7-402f-95fc-b8b987ec86c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Александр Сергеевич Пушкин\n",
            "\n",
            "                                Евгений Онегин\n",
            "                                Роман в стихах\n",
            "\n",
            "                        Не мысля гордый свет забавить,\n",
            "                        Вниманье дружбы возлюбя,\n",
            "                        Хотел бы я тебе представить\n",
            "                        Залог достойнее тебя,\n",
            "                        Достойнее души прекрасной,\n",
            "                        Святой исполненной мечты,\n",
            "                        Поэзии живой и ясной,\n",
            "                        Высоких дум и простоты;\n",
            "                        Но так и быть - рукой пристрастной\n",
            "                        Прими собранье пестрых глав,\n",
            "                        Полусмешных, полупечальных,\n",
            "                        Простонародных, идеальных,\n",
            "                        Небрежный плод моих забав,\n",
            "                        Бессонниц, легких вдохновений,\n",
            "                        Незрелых и увядших лет,\n",
            "                        Ума холодных наблюдений\n",
            "                        И сердца горестных замет.\n",
            "\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing"
      ],
      "metadata": {
        "id": "9VMxJY77dOsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text + text\n",
        "vocab = sorted(set(text))\n",
        "print(f'The unique characters is {len(vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKUmkirTVsMz",
        "outputId": "9ff2d346-ae0f-43a4-9f78-b696c17af0a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unique characters is 131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "metadata": {
        "id": "0y4n7bklWW5D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "WFhfCohsWv01"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_as_int)\n",
        "print(len(text_as_int))\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf68AHb5XcfJ",
        "outputId": "a552023f-ba0f-4f12-e090-c2b3669c29fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 71 110 104 ... 104 121   0]\n",
            "573968\n",
            "573968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "O_rZmxwcdJWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(9):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reMiu9zWYIFh",
        "outputId": "0bf9bf62-2509-42e1-d6e2-66581ab002d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А\n",
            "л\n",
            "е\n",
            "к\n",
            "с\n",
            "а\n",
            "н\n",
            "д\n",
            "р\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IC9iYQpeHLH",
        "outputId": "e89f7e7d-de8b-47f9-bfd6-1f83a8bda363"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
            "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
            "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
            "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
            "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "E-Lqr0T-e76e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vVgCfrwf3JC",
        "outputId": "14339193-8f8c-464f-e72c-62c388683202"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
            "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10_000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGtULwcngJnd",
        "outputId": "81e67645-d333-4a21-ccd6-eec7f2557e17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "dqbBQMoZgfqO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim),\n",
        "        LSTM(rnn_units, return_sequences=True),\n",
        "        Dense(vocab_size)])"
      ],
      "metadata": {
        "id": "ULYOFtimgo1N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNgenerator(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, batch_size):\n",
        "        super(RNNgenerator, self).__init__()\n",
        "\n",
        "        self.emb = Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.gru1 = GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform')\n",
        "        self.gru2 = GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        self.fc = Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        emb_x = self.emb(x)\n",
        "        x1 = self.gru1(emb_x)\n",
        "        x = x1\n",
        "        for _ in range(3):\n",
        "            x = self.gru2(x)\n",
        "        #x = self.gru1(x)\n",
        "        x = (x + x1) / 2\n",
        "        return self.fc(x)\n",
        "\n",
        "model = RNNgenerator(vocab_size, embedding_dim, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "STkaaCiqg9qv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim,\n",
        "                batch_input_shape=[batch_size, None]),\n",
        "\n",
        "        LSTM(rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        LSTM(rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        LSTM(rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        LSTM(rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "l_6Xhv8Alvy-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "                  vocab_size=len(vocab),\n",
        "                  embedding_dim=embedding_dim,\n",
        "                  rnn_units=rnn_units,\n",
        "                  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "WxVFB7W1mfwD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI9CxhZImo5r",
        "outputId": "82581b72-6412-4dfa-ed36-75de63d7ca83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 131) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-y8nW6Im6Xh",
        "outputId": "2e9a650d-45cc-41d3-cfb2-6951fa02d6f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (64, None, 128)           16768     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          4722688   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (64, None, 131)           134275    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30051843 (114.64 MB)\n",
            "Trainable params: 30051843 (114.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02g0LJ-KnDKf",
        "outputId": "53ea19c4-5252-4a49-8a36-5006c1be68cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 131), dtype=float32, numpy=\n",
              "array([[ 1.3410777e-05,  3.9764172e-06,  8.9904961e-06, ...,\n",
              "        -1.0529433e-05,  3.0581878e-06,  5.5118726e-06],\n",
              "       [ 3.7017799e-05,  6.5898244e-06,  2.7266677e-05, ...,\n",
              "        -1.6803517e-05,  2.6158745e-05,  1.3609933e-05],\n",
              "       [ 6.4786829e-05,  1.5839796e-05,  5.7312918e-05, ...,\n",
              "        -1.0970036e-05,  6.0849765e-05,  2.3962588e-05],\n",
              "       ...,\n",
              "       [-3.0246535e-03, -2.0020348e-03,  2.4522289e-03, ...,\n",
              "         1.4883170e-03,  6.9009094e-04,  8.7924981e-03],\n",
              "       [-3.0445270e-03, -2.0059932e-03,  2.4410463e-03, ...,\n",
              "         1.3925884e-03,  5.4525305e-04,  8.9247655e-03],\n",
              "       [-3.0567581e-03, -1.9996858e-03,  2.4180394e-03, ...,\n",
              "         1.3049056e-03,  3.9138179e-04,  9.0270815e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "74OdDm3KnJy6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print(\"\\nNext char predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKfK2tamovJG",
        "outputId": "659f456a-3ec8-4ff1-f694-281d9a37382a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " 'о б (не правда ли?) пора!\\n\\n                                     XLIX\\n\\n                        Кто б '\n",
            "\n",
            "Next char predictions: \n",
            " 'ЖэъФЬ\"ш ЦwЛ9IP8зоч{ЕДПъЦ2NтIдYр7t5 Г}иRTяA;Ю6DЮhоRmЬиЖAнmd\"щХHpeКAьБ-luЭ Кc9rП 060НПшыcИ4M2spСNQфBуb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "sOIy_qrupskI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW06XeurpYNo",
        "outputId": "45572ec7-0a9a-468e-89ae-dadbc240c2f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 131)  # (batch_size, sequence_length, vocab_size)\n",
            "Scalar_loss:       4.8758526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "fbIAIjaRqjkq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_freq=88 * 5,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "CppHaGnMqyyx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqj9eQ2rTkS",
        "outputId": "04fefce9-815e-42bb-a091-0a9db80f9c58"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "88/88 [==============================] - 42s 381ms/step - loss: 2.3651\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 38s 410ms/step - loss: 2.0201\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 36s 392ms/step - loss: 1.7007\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 36s 398ms/step - loss: 1.5349\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 37s 411ms/step - loss: 1.4351\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 1.3858\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 1.3409\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 1.3206\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 1.3214\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 40s 444ms/step - loss: 1.2892\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 1.3141\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 1.2706\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 36s 399ms/step - loss: 1.2560\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 1.2414\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 42s 471ms/step - loss: 1.2288\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 1.2262\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 1.2087\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 1.1767\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 1.1640\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 42s 470ms/step - loss: 1.1395\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 37s 413ms/step - loss: 1.1084\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 1.0850\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 1.0626\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 1.0609\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 43s 473ms/step - loss: 1.0411\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 37s 414ms/step - loss: 1.0101\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.9965\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 36s 398ms/step - loss: 0.9528\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.9582\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 43s 473ms/step - loss: 0.9219\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.8963\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.8895\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.8570\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 38s 409ms/step - loss: 0.8451\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 43s 475ms/step - loss: 0.8055\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 38s 414ms/step - loss: 0.7803\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.7648\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 0.7202\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.6845\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 43s 473ms/step - loss: 0.6509\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 37s 409ms/step - loss: 0.6172\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.5810\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.5475\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.5157\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 38s 419ms/step - loss: 0.5187\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.4613\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.4369\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.4319\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.3999\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 38s 418ms/step - loss: 0.3787\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.3673\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.3544\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.3463\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.3386\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 43s 473ms/step - loss: 0.3330\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 37s 409ms/step - loss: 0.3275\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.3255\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.3199\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.3158\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 38s 416ms/step - loss: 0.3466\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.3367\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.3141\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.3048\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.3096\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 43s 473ms/step - loss: 0.3017\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 38s 409ms/step - loss: 0.2957\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.2958\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2932\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2913\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 43s 475ms/step - loss: 0.2890\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2879\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.2855\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2843\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2811\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 43s 475ms/step - loss: 0.2800\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.2799\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2762\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 37s 402ms/step - loss: 0.2764\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2724\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 38s 419ms/step - loss: 0.2732\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2680\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2653\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 0.2643\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2627\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 39s 429ms/step - loss: 0.2611\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.2578\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2559\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2633\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2526\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 37s 414ms/step - loss: 0.2504\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2470\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 0.2447\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2434\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2399\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 42s 473ms/step - loss: 0.2370\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.2354\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.2303\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.2309\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.2268\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 38s 417ms/step - loss: 0.2254\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.2212\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.2186\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.2147\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.2110\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 42s 470ms/step - loss: 0.2101\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 38s 415ms/step - loss: 0.2084\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.2044\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 0.2036\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.1991\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1968\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.1952\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1906\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 37s 402ms/step - loss: 0.1918\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.1881\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 38s 418ms/step - loss: 0.1863\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1829\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1819\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.1795\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1764\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 43s 476ms/step - loss: 0.1755\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 37s 413ms/step - loss: 0.1734\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.1711\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 0.1723\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1695\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1679\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 37s 413ms/step - loss: 0.1659\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.1650\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 37s 402ms/step - loss: 0.1729\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.1686\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 43s 474ms/step - loss: 0.1637\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.1614\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1591\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1566\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1567\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 43s 475ms/step - loss: 0.1557\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.1555\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1553\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.1537\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.1530\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 43s 474ms/step - loss: 0.1524\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 37s 409ms/step - loss: 0.1524\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1527\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 0.1511\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1512\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 43s 474ms/step - loss: 0.1490\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 37s 407ms/step - loss: 0.1489\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1501\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1476\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 0.1472\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 43s 475ms/step - loss: 0.1479\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 38s 415ms/step - loss: 0.1484\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.1462\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 0.1458\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1456\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1450\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 37s 412ms/step - loss: 0.1439\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1446\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 0.1452\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 37s 410ms/step - loss: 0.1486\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 39s 429ms/step - loss: 0.1437\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1447\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.1428\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.1418\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1416\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1402\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 37s 412ms/step - loss: 0.1394\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1392\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 36s 398ms/step - loss: 0.1398\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 37s 409ms/step - loss: 0.1395\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1384\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 37s 411ms/step - loss: 0.1395\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1378\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1366\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 37s 408ms/step - loss: 0.1372\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 42s 468ms/step - loss: 0.1360\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 38s 418ms/step - loss: 0.1364\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.1370\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1359\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1366\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 41s 455ms/step - loss: 0.1368\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 37s 411ms/step - loss: 0.1355\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1365\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 0.1365\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 37s 409ms/step - loss: 0.1400\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 43s 472ms/step - loss: 0.1385\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 37s 412ms/step - loss: 0.1349\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1342\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 0.1342\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 37s 406ms/step - loss: 0.1339\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 42s 472ms/step - loss: 0.1329\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 38s 414ms/step - loss: 0.1319\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 0.1326\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 36s 402ms/step - loss: 0.1328\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1334\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 43s 474ms/step - loss: 0.1354\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 38s 415ms/step - loss: 0.1343\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 37s 403ms/step - loss: 0.1336\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 37s 404ms/step - loss: 0.1312\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 37s 405ms/step - loss: 0.1306\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 43s 472ms/step - loss: 0.1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text"
      ],
      "metadata": {
        "id": "94chzTWbrhYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vBApYgl_rjaU",
        "outputId": "6470bb81-ac47-488f-9a30-744dca1e78ff"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_200'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "gxBs5qRar9MI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssFtCCvDsbg5",
        "outputId": "bbf799fb-c8fe-43c9-b56c-2aa0e775fa2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (1, None, 128)            16768     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (1, None, 1024)           4722688   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (1, None, 131)            134275    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30051843 (114.64 MB)\n",
            "Trainable params: 30051843 (114.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1010\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.1\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "qBeg3p_VsiEv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = generate_text(model, start_string=\"Я помню чудное мгновенье\")\n",
        "print(text_)\n",
        "# print(len(text_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oztI2HU4slpc",
        "outputId": "7a0b0b32-fc13-48f6-8c11-9fcb9e0cea34"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Я помню чудное мгновенье\n",
            "                        Не ты ли, милое виденье,\n",
            "                        В прозрачной темноте мелькнул,\n",
            "                        Приникнул тихо к изголовью?\n",
            "                        Не ты ль, с отрадой и любовью,\n",
            "                        Слова надежды мне шепнул?\n",
            "                        Кто ты, мой ангел ли хранитель,\n",
            "                        Или коварный искуситель:\n",
            "                        Мои сомненья разреши.\n",
            "                        Быть может, это все пустое,\n",
            "                        Обман неопытной души!\n",
            "                        И суждено совсем иное...\n",
            "                        Но так и быть! Судьбу мою\n",
            "                        Отныне я тебе вручаю,\n",
            "                        Перед тобою слезы лью,\n",
            "                        Твоей защиты умоляю...\n",
            "                        Вообрази: я здесь одна,\n",
            "                        Никто меня не понимает,\n",
            "                        Рассудок мой изнемогает,\n",
            "                        И молча готовым он полунет,\n",
            "                        Без притязаний на успех,\n",
            "              \n"
          ]
        }
      ]
    }
  ]
}